# -*- coding: utf-8 -*-
"""convex_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LFZXWeIpFmUz2FffkLG0h7wHt1Y6SXyr
"""

# -*- coding: utf-8 -*-
"""
written by Ghazal Rahmanisane, Sahar Rezagholi

"""

import math
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.optim as optim  # will be patched below

# ---------------------------------------------------------------------------
# 0. Manual optimizer implementations
# ---------------------------------------------------------------------------
class _BaseOptim:
    def __init__(self, params):
        self.params = list(params)
    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.detach_()
                p.grad.zero_()

class ManualSGD(_BaseOptim):
    def __init__(self, params, lr=1e-3):
        super().__init__(params)
        self.lr = lr
    def step(self):
        for p in self.params:
            if p.grad is None:
                continue
            p.data.add_(p.grad, alpha=-self.lr)

class ManualAdaGrad(_BaseOptim):
    def __init__(self, params, lr=1e-2, eps=1e-10):
        super().__init__(params)
        self.lr, self.eps = lr, eps
        self.state = {id(p): torch.zeros_like(p) for p in self.params}
    def step(self):
        for p in self.params:
            if p.grad is None:
                continue
            st = self.state[id(p)]
            st.add_(p.grad.pow(2))
            adj = self.lr / (st.sqrt() + self.eps)
            p.data.addcmul_(p.grad, adj, value=-1.0)

class ManualRMSProp(_BaseOptim):
    def __init__(self, params, lr=1e-3, alpha=0.99, eps=1e-8):
        super().__init__(params)
        self.lr, self.alpha, self.eps = lr, alpha, eps
        self.state = {id(p): torch.zeros_like(p) for p in self.params}
    def step(self):
        for p in self.params:
            if p.grad is None:
                continue
            v = self.state[id(p)]
            v.mul_(self.alpha).addcmul_(p.grad, p.grad, value=1-self.alpha)
            denom = v.sqrt().add_(self.eps)
            p.data.addcdiv_(p.grad, denom, value=-self.lr)

class ManualAdam(_BaseOptim):
    def __init__(self, params, lr=1e-3, betas=(0.9,0.999), eps=1e-8):
        super().__init__(params)
        self.lr, self.beta1, self.beta2, self.eps = lr, *betas, eps
        self.state = {id(p): {'step':0, 'm':torch.zeros_like(p), 'v':torch.zeros_like(p)} for p in self.params}
    def step(self):
        for p in self.params:
            if p.grad is None:
                continue
            s = self.state[id(p)]
            g = p.grad
            s['step'] += 1
            t = s['step']
            s['m'].mul_(self.beta1).add_(g, alpha=1-self.beta1)
            s['v'].mul_(self.beta2).addcmul_(g, g, value=1-self.beta2)
            m_hat = s['m'] / (1 - self.beta1**t)
            v_hat = s['v'] / (1 - self.beta2**t)
            p.data.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)

# ---------------------------------------------------------------------------
# 1. Patch torch.optim to use manual implementations
# ---------------------------------------------------------------------------
import types as _types
import torch.optim as _orig_optim
_manual = _types.ModuleType('manual_optim')
_manual.SGD = ManualSGD
_manual.Adagrad = ManualAdaGrad
_manual.RMSprop = ManualRMSProp
_manual.Adam = ManualAdam
for name in ('SGD', 'Adagrad', 'RMSprop', 'Adam'):
    setattr(_orig_optim, name, getattr(_manual, name))
optim = _orig_optim

# ---------------------------------------------------------------------------
# 2. Utilities, model, and training loops
# ---------------------------------------------------------------------------
def get_device():
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class MLP(nn.Module):
    def __init__(self, input_dim=784, hidden1=200, hidden2=50, num_classes=10):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, hidden1), nn.ReLU(),
            nn.Linear(hidden1, hidden2), nn.ReLU(),
            nn.Linear(hidden2, num_classes)
        )
    def forward(self, x):
        return self.net(x)

# Training and evaluation

def train_one_epoch(model, device, loader, criterion, optimizer):
    model.train()
    total_loss = 0.0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x.size(0)
    return total_loss / len(loader.dataset)


def evaluate(model, device, loader):
    model.eval()
    correct = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            preds = model(x).argmax(dim=1)
            correct += (preds == y).sum().item()
    return correct / len(loader.dataset)

# ---------------------------------------------------------------------------
# 3. Experiment runners
# ---------------------------------------------------------------------------

def run_classification_experiments(ds, optimizers, epochs=10, batch_size=64):
    device = get_device()
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_ds = ds(root='data', train=True, download=True, transform=transform)
    test_ds  = ds(root='data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False)

    history = {}
    for name, factory in optimizers.items():
        torch.manual_seed(0)
        model = MLP().to(device)
        optimizer = factory(model.parameters())
        criterion = nn.CrossEntropyLoss()
        losses = []
        for _ in range(epochs):
            ep_loss = train_one_epoch(model, device, train_loader, criterion, optimizer)
            losses.append(ep_loss)
        acc = evaluate(model, device, test_loader)
        print(f"{ds.__name__} | {name:25s} -> Loss: {losses[-1]:.4f}, Acc: {acc*100:.2f}%")
        history[name] = (losses, acc)
    return history

# Synthetic objectives
class Quadratic(nn.Module):
    def __init__(self, A=None, b=None):
        super().__init__()
        self.name = 'Quadratic'
        A = torch.eye(2) if A is None else A
        b = torch.zeros(2) if b is None else b
        self.register_buffer('A', A)
        self.register_buffer('b', b)
    def forward(self, x):
        return 0.5 * x @ (self.A @ x) + self.b @ x

class Rosenbrock(nn.Module):
    def __init__(self, a=1.0, b=100.0):
        super().__init__()
        self.name = 'Rosenbrock'
        self.a = a
        self.b = b
    def forward(self, x):
        return (self.a - x[0])**2 + self.b * (x[1] - x[0]**2)**2

class Rastrigin(nn.Module):
    def __init__(self, A=10.0, dim=2):
        super().__init__()
        self.name = 'Rastrigin'
        self.A = A
        self.dim = dim
    def forward(self, x):
        return self.A * self.dim + (x**2 - self.A * torch.cos(2 * math.pi * x)).sum()


def run_synth_history(fn, init, optimizers, steps=500):
    device = get_device()
    fn = fn.to(device)
    history = {}
    for name, factory in optimizers.items():
        x = init.clone().detach().to(device).requires_grad_(True)
        optimizer = factory([x])
        losses = []
        for _ in range(steps):
            optimizer.zero_grad()
            loss = fn(x)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        print(f"{fn.name:10s} | {name:25s} -> Final loss: {losses[-1]:.6f}")
        history[name] = losses
    return history

# ---------------------------------------------------------------------------
# 4. Main: execute and plot with filtering
# ---------------------------------------------------------------------------
if __name__ == '__main__':
    epochs, steps = 10, 500
    beta_settings = [(0.9,0.999), (0.9,0.99), (0.5,0.999), (0.5,0.9), (0.9,0.5), (0.1,0.9)]
    adam_opts = {f"Adam β1={b1},β2={b2}": (lambda params, b1=b1, b2=b2: optim.Adam(params, lr=1e-3, betas=(b1, b2))) for b1, b2 in beta_settings}

    cls_opts = {
        'SGD lr=1e-3':     lambda p: optim.SGD(p, lr=1e-3),
        'AdaGrad lr=1e-2': lambda p: optim.Adagrad(p, lr=1e-2),
        'RMSProp lr=1e-3': lambda p: optim.RMSprop(p, lr=1e-3),
        **adam_opts
    }

    # Run classification
    mnist_hist   = run_classification_experiments(datasets.MNIST, cls_opts, epochs)
    fmnist_hist  = run_classification_experiments(datasets.FashionMNIST, cls_opts, epochs)

    # Filter and plot classification
    for title, hist in [('MNIST', mnist_hist), ('FashionMNIST', fmnist_hist)]:
        max_vals = sorted([max(losses) for losses, _ in hist.values()])
        thresh = max_vals[int(0.9 * (len(max_vals)-1))]
        plt.figure()
        for name, (losses, _) in hist.items():
            if max(losses) > thresh: continue
            plt.plot(range(1, epochs+1), losses, label=name)
        plt.title(f"{title} (filtered) Loss over Epochs")
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.yscale('log')
        plt.legend(fontsize='small', ncol=2)
        plt.tight_layout()
        plt.show()

    # Synthetic experiments
    synth_fns = [Quadratic(), Rosenbrock(), Rastrigin()]
    init_point = torch.tensor([2.0, -3.0])
    synth_opts = {
        'SGD lr=1e-2':     lambda p: optim.SGD(p, lr=1e-2),
        'AdaGrad lr=1e-2': lambda p: optim.Adagrad(p, lr=1e-2),
        'RMSProp lr=1e-3': lambda p: optim.RMSprop(p, lr=1e-3),
        **adam_opts
    }

    for fn in synth_fns:
        hist = run_synth_history(fn, init_point, synth_opts, steps)
        max_vals = sorted([max(l) for l in hist.values()])
        thresh = max_vals[int(0.9 * (len(max_vals)-1))]
        plt.figure()
        for name, losses in hist.items():
            if max(losses) > thresh: continue
            plt.plot(range(1, steps+1), losses, label=name)
        plt.title(f"{fn.name} (filtered) Loss over {steps} Steps")
        plt.xlabel('Step')
        plt.ylabel('Loss')
        plt.yscale('log')
        plt.legend(fontsize='small', ncol=2)
        plt.tight_layout()
        plt.show()

    print("\nAll filtered plots displayed.")

import matplotlib.pyplot as plt

# ─── 1) Manually enter your final test accuracies ───────────────────────────
mnist_acc = {
    'SGD lr=1e-3':           0.8806,   # e.g. 88.06% → 0.8806
    'AdaGrad lr=1e-2':       0.9620,
    'RMSProp lr=1e-3':       0.9712,
    'Adam β1=0.9,β2=0.999':   0.9744,
    'Adam β1=0.9,β2=0.99':    0.9774,
    'Adam β1=0.5,β2=0.999':   0.9758,
    'Adam β1=0.5,β2=0.9':     0.9703,
    'Adam β1=0.9,β2=0.5':     0.9233,
    'Adam β1=0.1,β2=0.9':     0.9769,
}

fashion_acc = {
    'SGD lr=1e-3':           0.8300,   # replace with your printed FashionMNIST numbers
    'AdaGrad lr=1e-2':       0.9000,
    'RMSProp lr=1e-3':       0.9200,
    'Adam β1=0.9,β2=0.999':   0.9300,
    'Adam β1=0.9,β2=0.99':    0.9400,
    'Adam β1=0.5,β2=0.999':   0.9350,
    'Adam β1=0.5,β2=0.9':     0.8500,
    'Adam β1=0.9,β2=0.5':     0.1000,
    'Adam β1=0.1,β2=0.9':     0.1050,
}

# ─── 2) Manually enter your final synthetic losses ───────────────────────────
synthetic_final = {
    'Quadratic': {
        'SGD lr=1e-2':         0.0012,
        'AdaGrad lr=1e-2':     0.0010,
        'RMSProp lr=1e-3':     0.0025,
        'Adam β1=0.9,β2=0.999': 0.0008,
        'Adam β1=0.9,β2=0.99':  0.0007,
        'Adam β1=0.5,β2=0.999': 0.0009,
        'Adam β1=0.5,β2=0.9':   0.0500,
        'Adam β1=0.9,β2=0.5':   1e6,
        'Adam β1=0.1,β2=0.9':   5e5,
    },
    'Rosenbrock': {
        'SGD lr=1e-2':         1234.56,
        'AdaGrad lr=1e-2':     234.56,
        'RMSProp lr=1e-3':     345.67,
        'Adam β1=0.9,β2=0.999': 456.78,
        'Adam β1=0.9,β2=0.99':  567.89,
        'Adam β1=0.5,β2=0.999': 678.90,
        'Adam β1=0.5,β2=0.9':   789.01,
        'Adam β1=0.9,β2=0.5':   8e4,
        'Adam β1=0.1,β2=0.9':   9e4,
    },
    'Rastrigin': {
        'SGD lr=1e-2':         12.345,
        'AdaGrad lr=1e-2':     23.456,
        'RMSProp lr=1e-3':     34.567,
        'Adam β1=0.9,β2=0.999': 45.678,
        'Adam β1=0.9,β2=0.99':  56.789,
        'Adam β1=0.5,β2=0.999': 67.890,
        'Adam β1=0.5,β2=0.9':   78.901,
        'Adam β1=0.9,β2=0.5':   8e3,
        'Adam β1=0.1,β2=0.9':   9e3,
    },
}

# ─── 3) Plot MNIST & FashionMNIST accuracies ────────────────────────────────
for title, results in (('MNIST', mnist_acc), ('FashionMNIST', fashion_acc)):
    plt.figure(figsize=(10,4))
    plt.bar(results.keys(), [v * 100 for v in results.values()])
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Accuracy (%)')
    plt.title(f'{title} Final Test Accuracies')
    plt.tight_layout()
    plt.show()

# ─── 4) Plot synthetic final losses (linear + log view) ───────────────────────
for fn_name, losses in synthetic_final.items():
    names, vals = list(losses.keys()), list(losses.values())

    # Linear‐scale
    plt.figure(figsize=(10,4))
    plt.bar(names, vals)
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Final Loss')
    plt.title(f'{fn_name} Final Losses (linear)')
    plt.tight_layout()
    plt.show()

    # Log‐scale
    plt.figure(figsize=(10,4))
    plt.bar(names, vals)
    plt.xticks(rotation=45, ha='right')
    plt.yscale('log')
    plt.ylim(bottom=min(v for v in vals if v>0) * 0.1, top=max(vals) * 10)
    plt.ylabel('Final Loss (log)')
    plt.title(f'{fn_name} Final Losses (log scale)')
    plt.tight_layout()
    plt.show()

import matplotlib.pyplot as plt

# ─────────────────────────────────────────────────────────────────────────────
# 1) Fill in your final test accuracies
# ─────────────────────────────────────────────────────────────────────────────
mnist_acc = {
    'SGD lr=1e-3':           0.8806,
    'AdaGrad lr=1e-2':       0.9620,
    'RMSProp lr=1e-3':       0.9712,
    'Adam β1=0.9,β2=0.999':   0.9744,
    'Adam β1=0.9,β2=0.99':    0.9774,
    'Adam β1=0.5,β2=0.999':   0.9758,
    'Adam β1=0.5,β2=0.9':     0.9703,
    'Adam β1=0.9,β2=0.5':     0.9233,
    'Adam β1=0.1,β2=0.9':     0.9769,
}

fashion_acc = {
    'SGD lr=1e-3':           0.8300,
    'AdaGrad lr=1e-2':       0.9000,
    'RMSProp lr=1e-3':       0.9200,
    'Adam β1=0.9,β2=0.999':   0.9300,
    'Adam β1=0.9,β2=0.99':    0.9400,
    'Adam β1=0.5,β2=0.999':   0.9350,
    'Adam β1=0.5,β2=0.9':     0.8500,
    'Adam β1=0.9,β2=0.5':     0.1000,
    'Adam β1=0.1,β2=0.9':     0.1050,
}

# ─────────────────────────────────────────────────────────────────────────────
# 2) Fill in your final synthetic losses for each objective
# ─────────────────────────────────────────────────────────────────────────────
losses = {
    'Quadratic': {
        'SGD lr=1e-2':         0.0012,
        'AdaGrad lr=1e-2':     0.0010,
        'RMSProp lr=1e-3':     0.0025,
        'Adam β1=0.9,β2=0.999': 0.0008,
        'Adam β1=0.9,β2=0.99':  0.0007,
        'Adam β1=0.5,β2=0.999': 0.0009,
        'Adam β1=0.5,β2=0.9':   0.0500,
        'Adam β1=0.9,β2=0.5':   1e6,
        'Adam β1=0.1,β2=0.9':   5e5,
    },
    'Rosenbrock': {
        'SGD lr=1e-2':         1.23,
        'AdaGrad lr=1e-2':     2.34,
        'RMSProp lr=1e-3':     3.45,
        'Adam β1=0.9,β2=0.999': 4.56,
        'Adam β1=0.9,β2=0.99':  5.67,
        'Adam β1=0.5,β2=0.999': 6.78,
        'Adam β1=0.5,β2=0.9':   7.89,
        'Adam β1=0.9,β2=0.5':   8e4,
        'Adam β1=0.1,β2=0.9':   9e4,
    },
    'Rastrigin': {
        'SGD lr=1e-2':         12.3,
        'AdaGrad lr=1e-2':     23.4,
        'RMSProp lr=1e-3':     34.5,
        'Adam β1=0.9,β2=0.999': 45.6,
        'Adam β1=0.9,β2=0.99':  56.7,
        'Adam β1=0.5,β2=0.999': 67.8,
        'Adam β1=0.5,β2=0.9':   78.9,
        'Adam β1=0.9,β2=0.5':   8e3,
        'Adam β1=0.1,β2=0.9':   9e3,
    },
}

# ─────────────────────────────────────────────────────────────────────────────
# 3) Scatter plots: for each synthetic fn, plot Quadratic/Rosen/Rastrigin loss vs both accuracies
# ─────────────────────────────────────────────────────────────────────────────
for fn_name, fn_losses in losses.items():
    plt.figure(figsize=(6,6))
    for name, loss_val in fn_losses.items():
        if name in mnist_acc:
            plt.scatter(loss_val, mnist_acc[name],
                        marker='o', s=80, label=f'{name} (MNIST)')
        if name in fashion_acc:
            plt.scatter(loss_val, fashion_acc[name],
                        marker='x', s=80, label=f'{name} (Fashion)')
    plt.xscale('log')
    plt.xlabel('Final ' + fn_name + ' Loss (log scale)')
    plt.ylabel('Test Accuracy')
    plt.title(f'{fn_name} Loss vs. Accuracy')
    plt.legend(fontsize='small', bbox_to_anchor=(1,1))
    plt.tight_layout()
    plt.show()

import torch
import torch.nn as nn
import math
from tqdm import tqdm

# 1) Re-define your synthetic objectives:
class Quadratic(nn.Module):
    def __init__(self):
        super().__init__()
        self.name = "Quadratic"
    def forward(self, x):
        return 0.5 * (x**2).sum()

class Rosenbrock(nn.Module):
    def __init__(self, a=1.0, b=100.0):
        super().__init__()
        self.name = "Rosenbrock"; self.a, self.b = a, b
    def forward(self, x):
        return (self.a - x[0])**2 + self.b * (x[1] - x[0]**2)**2

class Rastrigin(nn.Module):
    def __init__(self, A=10.0):
        super().__init__()
        self.name = "Rastrigin"; self.A = A
    def forward(self, x):
        return self.A * x.numel() + (x**2 - self.A * torch.cos(2 * math.pi * x)).sum()

# 2) Setup factories — bumped Adam LR to 1e-2 to trigger divergence
import torch.optim as optim
beta_settings = [(0.9,0.999),(0.9,0.99),(0.5,0.999),(0.5,0.9),(0.9,0.5),(0.1,0.9)]
adam_factories = {
    f"Adam β1={b1},β2={b2}": (lambda ps, b1=b1, b2=b2: optim.Adam(ps, lr=1e-2, betas=(b1,b2)))
    for b1,b2 in beta_settings
}
synth_opts = {
    "SGD lr=1e-2":     lambda ps: optim.SGD(ps, lr=1e-2),
    "AdaGrad lr=1e-2": lambda ps: optim.Adagrad(ps, lr=1e-2),
    "RMSProp lr=1e-3": lambda ps: optim.RMSprop(ps, lr=1e-3),
    **adam_factories
}

# 3) Ground-truth minimizers
true_min = {
    "Quadratic": torch.zeros(2),
    "Rosenbrock": torch.tensor([1.0,1.0]),
    "Rastrigin": torch.zeros(2),
}

# 4) Runner that returns (final_loss, error_norm)
def eval_synth(fn, init, factory, steps=500):
    x = init.clone().detach().requires_grad_(True)
    opt = factory([x])
    for _ in range(steps):
        opt.zero_grad()
        l = fn(x)
        l.backward()
        opt.step()
    loss = float(l.item())
    err  = float((x.detach() - true_min[fn.name]).norm().item())
    return loss, err

# 5) Run and print a table
init = torch.tensor([2.0, -3.0])
for fn in (Quadratic(), Rosenbrock(), Rastrigin()):
    print(f"\n=== {fn.name} ===")
    print(f"{'Opt':30s} | {'Loss':>12s} | {'‖x−x*‖':>8s}")
    print("-"*60)
    for name, fac in synth_opts.items():
        loss, err = eval_synth(fn, init, fac, steps=500)
        print(f"{name:30s} | {loss:12.4e} | {err:8.4f}")